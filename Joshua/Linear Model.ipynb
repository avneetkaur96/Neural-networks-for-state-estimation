{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer, TransformerDecoder\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import cont2discrete, lti, dlti, dstep\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import math\n",
    "import scipy.integrate\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=201):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class KalmanFormer(nn.Module):\n",
    "    def __init__(self, f, h, m, n, hidden_dim=64, num_heads=2, num_layers=2, dropout=0.1):\n",
    "        super(KalmanFormer, self).__init__()\n",
    "\n",
    "        self.f = f\n",
    "        self.h = h\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "\n",
    "        feature_dim_enc = 2 * n\n",
    "        feature_dim_dec = 2 * m\n",
    "\n",
    "        self.encoder_embedding = nn.Linear(feature_dim_enc, hidden_dim)\n",
    "        self.decoder_embedding = nn.Linear(feature_dim_dec, hidden_dim)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim)\n",
    "        self.pos_decoder = PositionalEncoding(hidden_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=num_heads, dim_feedforward=64,\n",
    "            dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim, nhead=num_heads, dim_feedforward=64,\n",
    "            dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.kalman_gain_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, m * n)\n",
    "        )\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def InitSequence(self, M1_0, T):\n",
    "        self.T = T\n",
    "        self.m1x_posterior = M1_0.to(self.device)\n",
    "        self.m1x_posterior_previous = self.m1x_posterior\n",
    "        self.m1x_prior_previous = self.m1x_posterior\n",
    "        self.y_previous = self.h(self.m1x_posterior)\n",
    "\n",
    "    def step_prior(self):\n",
    "        self.m1x_prior = self.f(self.m1x_posterior)\n",
    "        self.m1y = self.h(self.m1x_prior)\n",
    "\n",
    "    def compute_kalmanformer_inputs(self, x_post, x_post_prev, x_prior_prev, y_prior, y_previous, y):\n",
    "        F1 = (y - y_previous).squeeze(-1)\n",
    "        F2 = (y - y_prior).squeeze(-1)\n",
    "        F3 = (x_post - x_post_prev).squeeze(-1)\n",
    "        F4 = (x_post - x_prior_prev).squeeze(-1)\n",
    "\n",
    "        \n",
    "        F_encoder = torch.cat([F1, F2], dim=-1)  \n",
    "        F_decoder = torch.cat([F3, F4], dim=-1) \n",
    "\n",
    "        F_encoder = F.normalize(F_encoder, p=2, dim=1)\n",
    "        F_decoder = F.normalize(F_decoder, p=2, dim=1)\n",
    "\n",
    "        F_encoder = F_encoder.unsqueeze(1)\n",
    "        F_decoder = F_decoder.unsqueeze(1)\n",
    "\n",
    "\n",
    "        return F_encoder, F_decoder\n",
    "\n",
    "    def Kalman_step(self, y_t):\n",
    "        self.step_prior()\n",
    "\n",
    "        F_encoder, F_decoder = self.compute_kalmanformer_inputs(\n",
    "            self.m1x_posterior, self.m1x_posterior_previous,\n",
    "            self.m1x_prior_previous, self.m1y, self.y_previous, y_t\n",
    "        )\n",
    "\n",
    "        enc = self.encoder_embedding(F_encoder)\n",
    "        enc = self.pos_encoder(enc)\n",
    "        memory = self.encoder(enc)\n",
    "\n",
    "        dec = self.decoder_embedding(F_decoder)\n",
    "        dec = self.pos_decoder(dec)\n",
    "        decoded = self.decoder(dec, memory)\n",
    "\n",
    "        kalman_gain = self.kalman_gain_head(decoded).squeeze(1)\n",
    "        kalman_gain = kalman_gain.view(-1, self.m, self.n)\n",
    "\n",
    "        dy = y_t - self.m1y\n",
    "        INOV = torch.bmm(kalman_gain, dy)\n",
    "\n",
    "        self.m1x_posterior_previous = self.m1x_posterior\n",
    "        self.m1x_posterior = self.m1x_prior + INOV\n",
    "        self.m1x_prior_previous = self.m1x_prior\n",
    "        self.y_previous = y_t\n",
    "\n",
    "        return self.m1x_posterior\n",
    "\n",
    "    def forward(self, y_sequence):\n",
    "        batch_size, seq_len, _, _ = y_sequence.shape\n",
    "        preds = []\n",
    "        for t in range(seq_len):\n",
    "            y_t = y_sequence[:, t, :, :]\n",
    "            x_hat = self.Kalman_step(y_t)\n",
    "            preds.append(x_hat.unsqueeze(1))\n",
    "\n",
    "        return torch.cat(preds, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 100, 4)\n"
     ]
    }
   ],
   "source": [
    "def data_generation(num_sequences, sequence_length, number_masses):\n",
    "    dim_y = number_masses\n",
    "    dim_x= 2*dim_y \n",
    "\n",
    "    X_data_array = np.empty((num_sequences, sequence_length, dim_x))\n",
    "    Y_data_array = np.empty((num_sequences, sequence_length, dim_y))\n",
    "\n",
    "    m = np.ones(dim_y)\n",
    "    m = 10*m\n",
    "   \n",
    "    k = np.ones(dim_y)\n",
    "    k = 800*k\n",
    "\n",
    "    d = np.ones(dim_y)\n",
    "    d = 6*d\n",
    "\n",
    "    A_c = np.zeros((dim_x,dim_x))\n",
    "\n",
    "    offset = 0\n",
    "    for i in range(dim_x):\n",
    "        if i % 2 == 0:\n",
    "            A_c[i,i+1] = 1\n",
    "\n",
    "        if i % 2 == 1:\n",
    "            if i != dim_x-1:\n",
    "                A_c[i,i-1] = -(k[i-1-offset]+k[i-offset])/m[i-1-offset]\n",
    "                A_c[i,i] = -(d[i-1-offset]+d[i-offset])/m[i-1-offset]\n",
    "                A_c[i,i+1] = k[i-offset]/m[i-1-offset]\n",
    "                A_c[i,i+2] = d[i-offset]/m[i-1-offset]\n",
    "            else:\n",
    "                A_c[i,i-1] = -k[i-dim_y]/m[i-dim_y]\n",
    "                A_c[i,i] = -d[i-dim_y]/m[i-dim_y]\n",
    "\n",
    "            if i != 1:\n",
    "                A_c[i,i-3] = k[i-1-offset]/m[i-1-offset]\n",
    "                A_c[i,i-2] = d[i-1-offset]/m[i-1-offset]\n",
    "\n",
    "            offset += 1\n",
    "\n",
    "    B_c = np.zeros((dim_x,dim_y))\n",
    "\n",
    "    H_c = np.zeros((dim_y,dim_x))\n",
    "    offset = 0\n",
    "    for i in range(dim_y):\n",
    "        H_c[i,i+offset] = 1\n",
    "\n",
    "        offset += 1\n",
    "\n",
    "    D_c = np.array([[0.]])\n",
    "\n",
    "   \n",
    "    dt = 0.1 \n",
    "    d_system = cont2discrete((A_c, B_c, H_c, D_c),dt)\n",
    "    A = d_system[0] \n",
    "    H = d_system[2] \n",
    "\n",
    "    def is_schur(matrix):\n",
    "       \n",
    "        eigenvalues, _ = np.linalg.eig(matrix)\n",
    "        if np.all(np.abs(eigenvalues) < 1):\n",
    "            print(np.abs(eigenvalues))\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # if is_schur(A):\n",
    "    #     print(\"The matrix is Schur.\")\n",
    "    # else:\n",
    "    #     print(\"The matrix is not Schur.\")\n",
    "\n",
    "\n",
    "    sigma_p = 0.01 \n",
    "    sigma_p_diag = (sigma_p**2)*np.ones(dim_x)\n",
    "    Q = np.diag(sigma_p_diag)\n",
    "\n",
    "    sigma_m = 0.01 \n",
    "    sigma_m_diag = (sigma_m**2)*np.ones(dim_y)\n",
    "    R = np.diag(sigma_m_diag)\n",
    "\n",
    "    sigma_x = 0.01 \n",
    "    sigma_x_diag = (sigma_p**2)*np.ones(dim_x)\n",
    "    P = np.diag(sigma_x_diag)\n",
    "\n",
    "\n",
    "    for s in range(num_sequences):\n",
    "        mu_x0 = np.random.uniform(-10,10,size=dim_x) \n",
    "        x=np.random.multivariate_normal(mu_x0,P)\n",
    "       \n",
    "        X_data_array[s,0,:] = np.squeeze(np.asarray(x))\n",
    "\n",
    "       \n",
    "        v_0 = np.random.multivariate_normal(np.zeros(dim_y),R).reshape(-1,1)\n",
    "\n",
    "        y = H.dot(x.reshape(-1,1)) + v_0\n",
    "\n",
    "\n",
    "        W = np.random.multivariate_normal(np.zeros(dim_x), Q, sequence_length)\n",
    "        V = np.random.multivariate_normal(np.zeros(dim_y), R, sequence_length)\n",
    "        Y_data_array[s,0,:] = np.squeeze(np.asarray(y))\n",
    "\n",
    "        for t in range(1,sequence_length+1):\n",
    "            w = W[t-1].reshape(-1,1) \n",
    "            v = V[t-1].reshape(-1,1)\n",
    "        \n",
    "            x = A.dot(x.reshape(-1,1)) + w \n",
    "            y = H.dot(x.reshape(-1,1)) + v \n",
    "\n",
    "            X_data_array[s,t:t+1,:] = x.T\n",
    "            Y_data_array[s,t:t+1,:] = y.T\n",
    "\n",
    "    return X_data_array, Y_data_array, A, H\n",
    "\n",
    "print(data_generation(200, 100, 2)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loaders(X_np, Y_np, batch_size, train_ratio, val_ratio):\n",
    "   \n",
    "  \n",
    "    X = torch.tensor(X_np, dtype=torch.float32).unsqueeze(-1)  \n",
    "    Y = torch.tensor(Y_np, dtype=torch.float32).unsqueeze(-1)  \n",
    "\n",
    "    dataset = TensorDataset(Y, X)\n",
    "\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    val_size = int(val_ratio * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    train_data = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_data = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_data = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kalmanformer(model, data_loader, optimizer, epochs):\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
    "    model.train()\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for y_seq, x_true in data_loader:\n",
    "            _, T, n, _ = y_seq.shape\n",
    "\n",
    "            x0 = torch.zeros_like(x_true[:, 0, :, :])\n",
    "            model.InitSequence(x0, T)\n",
    "\n",
    "#             g, i, u = model(y_seq[:, 1:], return_gain=True)\n",
    "#      \n",
    "#             K = torch.stack(g, dim=1)          \n",
    "#             dz = torch.stack(i, dim=1)    \n",
    "#             dx = torch.stack(u, dim=1)       \n",
    "#             pred_dx = torch.matmul(K, dz)           \n",
    "           \n",
    "#             loss = F.mse_loss(pred_dx, dx)    \n",
    "       \n",
    "            preds = model(y_seq[:, 1:])  \n",
    "\n",
    "            loss = F.mse_loss(preds, x_true[:, 1:, :, :]) \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {total_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 500, 2, 1])\n",
      "torch.Size([60, 500, 2, 1])\n",
      "torch.Size([40, 500, 2, 1])\n",
      "Epoch 1: Loss = 32.454988\n",
      "torch.Size([60, 500, 2, 1])\n",
      "torch.Size([60, 500, 2, 1])\n",
      "torch.Size([40, 500, 2, 1])\n",
      "Epoch 2: Loss = 30.094148\n",
      "torch.Size([60, 500, 2, 1])\n",
      "torch.Size([60, 500, 2, 1])\n",
      "torch.Size([40, 500, 2, 1])\n",
      "Epoch 3: Loss = 28.035199\n",
      "torch.Size([60, 500, 2, 1])\n",
      "torch.Size([60, 500, 2, 1])\n",
      "torch.Size([40, 500, 2, 1])\n",
      "Epoch 4: Loss = 26.107437\n",
      "torch.Size([60, 500, 2, 1])\n",
      "torch.Size([60, 500, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_np, Y_np, A, H = data_generation(num_sequences=200, sequence_length = 500, number_masses = 1)\n",
    "\n",
    "A = torch.tensor(A, dtype=torch.float32)\n",
    "H = torch.tensor(H, dtype=torch.float32)\n",
    "\n",
    "def f(x): return A @ x\n",
    "def h(x): return H @ x\n",
    "    \n",
    "import time\n",
    "start = time.time(\n",
    ")\n",
    "model = KalmanFormer(f=f, h=h, m=A.shape[0], n=H.shape[0], hidden_dim=64, num_heads=4, num_layers=2)\n",
    "\n",
    "train_data, val_data, test_data = data_loaders(X_np, Y_np, batch_size=60, train_ratio =0.8, val_ratio =0.1)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay = 0.001)\n",
    "train_kalmanformer(model, train_data, optimizer, epochs = 20)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kalmanformer_true_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_seqs = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        y_seq, x_true = batch  \n",
    "        b, T, m, _ = x_true.shape\n",
    "        x0 = torch.zeros_like(x_true[:, 0, :, :])  \n",
    "#         x0 = x_true[:, 0, :, :]  \n",
    "        model.InitSequence(x0, T)\n",
    "\n",
    "        x_pred = model(y_seq[:, 1:])   \n",
    "    \n",
    "\n",
    "        x_gt = x_true[:, 1:, :, :]           \n",
    "#         print(x_pred[0] - x_gt[0])\n",
    "        loss = F.mse_loss(x_pred, x_gt)  \n",
    "        total_loss += loss.item()\n",
    "#         total_seqs += b  \n",
    "\n",
    "\n",
    "    final_mse = total_loss \n",
    "    print(f\"[MSE] = {final_mse:.6f}\")\n",
    "    return final_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_kalmanformer_true_mse(model, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_vs_true_simple(model, batch, title_prefix=\"Train\"):\n",
    "    model.eval()\n",
    "    y_seq, x_true = batch\n",
    "    x0 = torch.zeros_like(x_true[:, 0, :, :])  \n",
    "#     x0 = x_true[:, 0, :, :]\n",
    "    T = y_seq.shape[1]\n",
    "\n",
    "    model.InitSequence(x0, T)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x_pred = model(y_seq[:, 1:]) \n",
    "\n",
    "    x_true = x_true[:, 1:, :, :]     \n",
    "\n",
    "    idx = 5\n",
    "    pred_np = x_pred[idx].squeeze(-1).cpu().numpy()  \n",
    "    true_np = x_true[idx].squeeze(-1).cpu().numpy()  \n",
    "\n",
    "    time = np.arange(pred_np.shape[0])\n",
    "    for state in range(pred_np.shape[1]-1):\n",
    "        plt.plot(time, true_np[:, state], label=f'True State {idx}')\n",
    "        plt.plot(time, pred_np[:, state], '--', label=f'Pred State {idx}')\n",
    "\n",
    "    plt.title(f'{title_prefix} Trajectory')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('State')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot_pred_vs_true_simple(model, next(iter(train_data)), title_prefix=\"Train\")\n",
    "plot_pred_vs_true_simple(model, next(iter(test_data)), title_prefix=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
