{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "259c98f5",
   "metadata": {},
   "source": [
    "Step 1: We import stuff that we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3796da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Imported essential modules and functionality\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer, TransformerDecoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.optim import Adam\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "print(\"###Imported essential modules and functionality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f437c894",
   "metadata": {},
   "source": [
    "Step 2: We now define the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b36cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Defined Class TransformerAutoEncoder\n"
     ]
    }
   ],
   "source": [
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder_input_dim, \n",
    "                 decoder_input_dim, \n",
    "                 hidden_dim,\n",
    "                 num_heads, \n",
    "                 encoder_embedding_dim, \n",
    "                 decoder_embedding_dim,\n",
    "                 num_layers, \n",
    "                 dropout):\n",
    "        super(TransformerAutoencoder, self).__init__()\n",
    "        self.encoder_input_dim = encoder_input_dim\n",
    "        self.decoder_input_dim = decoder_input_dim\n",
    "        self.encoder_embedding_dim = encoder_embedding_dim\n",
    "        self.decoder_embedding_dim = decoder_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "        # Encoder Embedding\n",
    "        self.encoder_embedding = nn.Linear(self.encoder_input_dim, self.encoder_embedding_dim)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model=self.encoder_embedding_dim,\n",
    "                                                     nhead=self.num_heads,\n",
    "                                                     dim_feedforward=self.hidden_dim,\n",
    "                                                     dropout=self.dropout,\n",
    "                                                     batch_first=True)\n",
    "        self.encoder = TransformerEncoder(self.encoder_layer,\n",
    "                                          num_layers=self.num_layers)\n",
    "\n",
    "        # Decoder Embedding\n",
    "        self.decoder_embedding = nn.Linear(self.decoder_input_dim, self.decoder_embedding_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_layer = TransformerDecoderLayer(d_model=self.decoder_embedding_dim,\n",
    "                                                     nhead=self.num_heads,\n",
    "                                                     dim_feedforward=self.hidden_dim,\n",
    "                                                     dropout=self.dropout,\n",
    "                                                     batch_first=True)\n",
    "        self.decoder = TransformerDecoder(self.decoder_layer, num_layers=self.num_layers)\n",
    "\n",
    "        # Final output layer\n",
    "        self.out = nn.Linear(self.decoder_embedding_dim, self.decoder_input_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Use the mask to create a version of x with missing values set to zero.\n",
    "        \n",
    "        # Encode the masked input.\n",
    "        x = self.encoder_embedding(x)\n",
    "        x_encoded = self.encoder(x)\n",
    "\n",
    "        # Decode the encoded representation.\n",
    "        y = self.decoder_embedding(y)\n",
    "        x_decoded = self.decoder(y, x_encoded)\n",
    "\n",
    "        # Apply the final output layer\n",
    "        x_out = self.out(x_decoded)\n",
    "\n",
    "        return x_out\n",
    "print(\"###Defined Class TransformerAutoEncoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de411c",
   "metadata": {},
   "source": [
    "Step 3: We then import the data necessary excluding the initial condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "341db9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Loaded data\n"
     ]
    }
   ],
   "source": [
    "datachoice = \"Lorenz\" # $$ acceptable values Lorrentz, KS, Burgers\n",
    "\n",
    "if datachoice==\"Lorenz\":\n",
    "    datafile = \"lorenz_data_hackathon.npz\" # also there is \"Data/lorenz_data_diff_ic.npz\"\n",
    "    \n",
    "\n",
    "# elif datachoice==\"KS\":\n",
    "#     datafile = \"Data/ks_data.npz\"\n",
    "#     # TODO\n",
    "# elif datachoice==\"burgers\":\n",
    "#     datafile= \"Data/burgers_data.npz\"\n",
    "#     # TODO\n",
    "\n",
    "\n",
    "rawData = np.load(datafile)\n",
    "allInputs = torch.Tensor(rawData[\"Y_data\"])\n",
    "allTargets = torch.Tensor(rawData[\"X_data\"])\n",
    "print(\"## Loaded data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41893b8",
   "metadata": {},
   "source": [
    "Step 4: We do the splitting of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d4c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct it\n",
    "\n",
    "trainPercent = 0.8\n",
    "testPercent = 0.1\n",
    "validatePercent = 0.1\n",
    "splitDataList = random_split(gridDataSet, [trainPercent, testPercent, validatePercent], torch.Generator().manual_seed(42))\n",
    "\n",
    "trainingData = splitDataList[0]\n",
    "testingData = splitDataList[1]\n",
    "validatingData = splitDataList[2]\n",
    "lenTrainingData = len(trainingData)\n",
    "lenTestingData = len(testingData)\n",
    "lenValidatingData = len(validatingData)\n",
    "print(\"## Split the data:\\n\")\n",
    "print(f\"Training data set size   : {lenTrainingData}\\n\",\n",
    "      f\"Validation data set size : {lenTestingData}\\n\",\n",
    "      f\"Test data set size       : {lenValidatingData}\")\n",
    "\n",
    "trainingTensorDataSet = TensorDataset(trainingData.dataset.inputs,\n",
    "                                      trainingData.dataset.targets)\n",
    "testingTensorDataSet = TensorDataset(testingData.dataset.inputs,\n",
    "                                      testingData.dataset.targets)\n",
    "validatingTensorDataSet = TensorDataset(validatingData.dataset.inputs,\n",
    "                                      validatingData.dataset.targets)\n",
    "\n",
    "print(trainingData[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateLossFromTrainingData(train_dataloader, criterion):\n",
    "    sum_loss = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        inputs, targets = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, targets)\n",
    "        loss = criterion(outputs, targets)\n",
    "        sum_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss = sum_loss/len(train_dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab1f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Store best results\n",
    "best_result = dict({\n",
    "    'model':[],\n",
    "    'learn_rate':[],\n",
    "    'num_epochs':[],\n",
    "    'encoder_embedding_dim':[],\n",
    "    'decoder_embedding_dim':[],\n",
    "    'hidden_dim':[],\n",
    "    'num_heads':[],\n",
    "    'weight_decay':[],\n",
    "    'dropout':[],\n",
    "    'avg_training_loss':[],\n",
    "    'best_validation_loss':[]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7bab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define constants\n",
    "encoder_input_dim = gridDataSet.input_dim\n",
    "decoder_input_dim = gridDataSet.output_dim\n",
    "output_dim = gridDataSet.output_dim\n",
    "num_layers = 4\n",
    "parameters = dict(num_epochs = [64],\n",
    "                  hidden_dim = [64],\n",
    "                  num_heads = [4],\n",
    "                  encoder_embedding_dim = [16],\n",
    "                  decoder_embedding_dim = [16],\n",
    "                  learn_rate = [0.1],\n",
    "                  weight_decay = [0.005],\n",
    "                  dropout = [0.05])\n",
    "\n",
    "param_values = [v for v in parameters.values()]\n",
    "\n",
    "for run_id, (num_epochs,\n",
    "             hidden_dim,\n",
    "             num_heads,\n",
    "             encoder_embedding_dim, \n",
    "             decoder_embedding_dim,\n",
    "             learn_rate,\n",
    "             weight_decay, \n",
    "             dropout) in enumerate(product(*param_values)):\n",
    "    criterion = nn.MSELoss()\n",
    "    trainDataSetLoader =  DataLoader(trainingTensorDataSet, batch_size = 5, shuffle = True)\n",
    "    validationDataSetLoader = DataLoader(validatingTensorDataSet, batch_size = 5, shuffle = True)\n",
    "\n",
    "    model = TransformerAutoencoder(encoder_input_dim, \n",
    "                                   decoder_input_dim,\n",
    "                                   hidden_dim, \n",
    "                                   num_heads, \n",
    "                                   encoder_embedding_dim,\n",
    "                                   decoder_embedding_dim, \n",
    "                                   num_layers, \n",
    "                                   dropout)\n",
    "    optimizer = Adam(model.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "    best_validation_loss = 1_000_000.\n",
    "    avg_training_loss = 0.\n",
    "    avg_validation_loss = 0.\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"EPOCH NUMBER: {epoch+1}\")\n",
    "        model.train(True)\n",
    "        avg_training_loss = evaluateLossFromTrainingData(trainDataSetLoader, criterion)\n",
    "        model.eval()\n",
    "        sum_validation_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(validationDataSetLoader):\n",
    "                inputs, targets = batch\n",
    "                outputs = model(inputs, targets)\n",
    "                val_loss = criterion(outputs, targets)\n",
    "                sum_validation_loss += val_loss.item()\n",
    "        avg_validation_loss = sum_validation_loss / len(validationDataSetLoader)\n",
    "        print(f\"AVERAGE TRAINING LOSS  : {avg_training_loss}\\nAVERAGE VALIDATION LOSS: {avg_validation_loss}\")\n",
    "\n",
    "        if avg_validation_loss < best_validation_loss:\n",
    "            best_validation_loss = avg_validation_loss\n",
    "            print(f'BEST VALIDATION LOSS: {best_validation_loss} at EPOCH {epoch+1}')\n",
    "            best_result['model']=model.state_dict()\n",
    "            best_result['learn_rate']=learn_rate\n",
    "            best_result['encoder_embedding_dim']=encoder_embedding_dim\n",
    "            best_result['decoder_embedding_dim'] =decoder_embedding_dim\n",
    "            best_result['num_epochs']=num_epochs\n",
    "            best_result['hidden_dim']=hidden_dim\n",
    "            best_result['num_heads']=num_heads\n",
    "            best_result['weight_decay']=weight_decay\n",
    "            best_result['dropout']=dropout\n",
    "            best_result['avg_training_loss']=avg_training_loss\n",
    "            best_result['best_validation_loss']=best_validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84765b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss_model = TransformerAutoencoder(encoder_input_dim, \n",
    "                                        decoder_input_dim, \n",
    "                                        hidden_dim, \n",
    "                                        num_heads,\n",
    "                                        encoder_embedding_dim, \n",
    "                                        decoder_embedding_dim, \n",
    "                                        num_layers, \n",
    "                                        dropout)\n",
    "\n",
    "min_loss_model.load_state_dict(best_result[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1274ef",
   "metadata": {},
   "source": [
    "Step 5: Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a23fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataSetLoader, criterion):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    sum_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataSetLoader:\n",
    "            inputs, targets = batch\n",
    "            prediction = model(inputs, targets)\n",
    "            predictions.append(prediction)\n",
    "            labels.append(targets)\n",
    "            test_loss = criterion(prediction,targets)\n",
    "            sum_loss+=test_loss\n",
    "        avg_test_loss = sum_loss/len(dataSetLoader)\n",
    "    \n",
    "    print(f\"AVERATE TEST LOSS: {avg_test_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c592d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "predict(min_loss_model, testingTensorDataSet,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5011ca40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
